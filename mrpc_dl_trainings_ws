# siamese with one hidden layer (768) and tanh activation
# mean of ALL embeddings
# Epoch: 18
Accuracy: 0.76690+-0.03268
F1: 0.83254+-0.02580

# CLS classification with sigmoid on bert-base-cased-mrpc (w/o dropout)
Epoch:1
Accuracy: 0.83008+-0.03464
F1: 0.87448+-0.02778

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 768, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83182+-0.03472
F1: 0.87617+-0.02747

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83298+-0.03584
F1: 0.87697+-0.02840


# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 512
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:2
Accuracy: 0.83298+-0.03479
F1: 0.87696+-0.02773

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 16, base_model os trainable, batch_size=16, epochs=100, epoch_size=16
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 18
Accuracy: 0.83818+-0.03178
F1: 0.88113+-0.02572

Epoch: 25
Accuracy: 0.84340+-0.03496
F1: 0.88564+-0.02730


# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 16, base_model os trainable, batch_size=12, epochs=100, epoch_size=80
mean of logits [s1, s2] and [s2, s1]
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 11
Accuracy: 0.84981+-0.02309
F1: 0.88718+-0.01881

Epoch: 14
Accuracy: 0.85157+-0.01803
F1: 0.88886+-0.01670

Epoch: 24
Accuracy: 0.85039+-0.02438
F1: 0.89079+-0.01925

Epoch: 36
Accuracy: 0.85039+-0.02132
F1: 0.88936+-0.01855

# fixed seed 666
Epoch: 9
Accuracy: 0.85444+-0.02743
F1: 0.89463+-0.02125

# bert-base-cased
Epoch: 13
Accuracy: 0.85213+-0.02504
F1: 0.88925+-0.02071

Epoch: 18
Accuracy: 0.85097+-0.02680
F1: 0.89042+-0.02160

Epoch: 25
Accuracy: 0.85270+-0.02480
F1: 0.89090+-0.01978