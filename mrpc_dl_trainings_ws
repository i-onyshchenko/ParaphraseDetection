torch==1.3.0

# siamese with one hidden layer (768) and tanh activation
# mean of ALL embeddings
# Epoch: 18
Accuracy: 0.76690+-0.03268
F1: 0.83254+-0.02580

# CLS classification with sigmoid on bert-base-cased-mrpc (w/o dropout)
Epoch:1
Accuracy: 0.83008+-0.03464
F1: 0.87448+-0.02778

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 768, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83182+-0.03472
F1: 0.87617+-0.02747

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83298+-0.03584
F1: 0.87697+-0.02840


# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 512
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:2
Accuracy: 0.83298+-0.03479
F1: 0.87696+-0.02773

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 16, base_model os trainable, batch_size=16, epochs=100, epoch_size=16
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 18
Accuracy: 0.83818+-0.03178
F1: 0.88113+-0.02572

Epoch: 25
Accuracy: 0.84340+-0.03496
F1: 0.88564+-0.02730


# CLS classification with sigmoid on bert-base-cased_mrpc, hidden = 768, base_model is trainable, batch_size=12, epochs=100, epoch_size=80
mean of logits [s1, s2] and [s2, s1]
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 11
Accuracy: 0.84981+-0.02309
F1: 0.88718+-0.01881

Epoch: 14
Accuracy: 0.85157+-0.01803
F1: 0.88886+-0.01670

Epoch: 24
Accuracy: 0.85039+-0.02438
F1: 0.89079+-0.01925

Epoch: 36
Accuracy: 0.85039+-0.02132
F1: 0.88936+-0.01855

# fixed seed 666
Epoch: 9
Accuracy: 0.85444+-0.02743
F1: 0.89463+-0.02125

# bert-base-cased
Epoch: 13
Accuracy: 0.85213+-0.02504
F1: 0.88925+-0.02071

Epoch: 18
Accuracy: 0.85097+-0.02680
F1: 0.89042+-0.02160

Epoch: 25
Accuracy: 0.85270+-0.02480
F1: 0.89090+-0.01978

# after update of transformers
Epoch: 14
Accuracy: 0.85444+-0.03050
F1: 0.89368+-0.02368

# bert-base with head_pretraining (same sched and optim),lr=0.01, momentum=0.9, nesterov, weight_decay=0.01
Epoch: 19
Accuracy: 0.85910+-0.02104
F1: 0.89200+-0.01820

Epoch: 21
Accuracy: 0.86025+-0.02087
F1: 0.89232+-0.01905

# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 28
Accuracy: 0.86201+-0.02340
F1: 0.89648+-0.02057

# mean aggregation
# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9


# max aggregation
# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 27
Accuracy: 0.85969+-0.02440
F1: 0.89701+-0.02076

# CLS aggregation, logistic, empty head
# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 24
Accuracy: 0.86375+-0.02246
F1: 0.89817+-0.01868
Best threshold: 0.52740+-0.03342

Epoch: 30
Accuracy: 0.86199+-0.02568
F1: 0.89483+-0.02298
Best threshold: 0.67940+-0.01183

##############################################################################################################

# baseline cos_dist for bert-base
Accuracy: 0.66371+-0.03432
F1: 0.79724+-0.02451
Best threshold: 0.11500+-0.00438

# siam, m = 0.5
# bert-base lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.69736+-0.02081
F1: 0.80069+-0.01362
Best threshold: 0.01290+-0.00030

Epoch: 14
Accuracy: 0.69849+-0.02362
F1: 0.79851+-0.01736
Best threshold: 0.01200+-0.00000

Epoch: 24
Accuracy: 0.71069+-0.02533
F1: 0.79608+-0.02179
Best threshold: 0.01000+-0.00000

# siam triplet, m = 0.5
# bert-base lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.72867+-0.02042
F1: 0.81817+-0.01671
Best threshold: 0.36500+-0.00000

# semi-siam
# bert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 27
Accuracy: 0.72748+-0.03515
F1: 0.81368+-0.02718
Best threshold: 0.56600+-0.00000

# semi-siam, but cos_dist used for evaluation
# bert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 25
Accuracy: 0.68055+-0.01849
F1: 0.78756+-0.01693
Best threshold: 0.08670+-0.00283

########################################################

# baseline cos_dist for bert-base-mrpc-pretrained
Accuracy: 0.74896+-0.03613
F1: 0.82616+-0.02830
Best threshold: 0.06700+-0.00000

# siam, m = 0.5
# bert-base-mrpc-pretrained lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 11
Accuracy: 0.76811+-0.03176
F1: 0.84004+-0.02485
Best threshold: 0.03900+-0.00000

Epoch: 23
Accuracy: 0.76924+-0.03893
F1: 0.83710+-0.02855
Best threshold: 0.03400+-0.00077

# siam triplet, m = 0.5
# bert-base-mrpc-pretrained lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 11
Accuracy: 0.74373+-0.03108
F1: 0.82526+-0.02322
Best threshold: 0.35120+-0.00040

# semi-siam
# bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 13
Accuracy: 0.77970+-0.02616
F1: 0.84483+-0.02159
Best threshold: 0.47040+-0.00120

Epoch: 18
Accuracy: 0.78029+-0.02628
F1: 0.83889+-0.02162
Best threshold: 0.54620+-0.00098

# semi-siam, but cos_dist used for evaluation
# bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 23
Accuracy: 0.76694+-0.03105
F1: 0.84074+-0.02244
Best threshold: 0.14900+-0.00000

####################
# baseline cos_dist for my bert-base-pretrained
Accuracy: 0.70782+-0.01237
F1: 0.78959+-0.01174
Best threshold: 0.08820+-0.00792

# siam, m = 0.5
# my bert-base-pretrained, lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 9
Accuracy: 0.74602+-0.03152
F1: 0.83104+-0.02335
Best threshold: 0.04810+-0.00030

Epoch: 23
Accuracy: 0.75065+-0.03499
F1: 0.82807+-0.02552
Best threshold: 0.03420+-0.00060

# siam triplet, m = 0.5
# my bert-base-pretrained lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 14
Accuracy: 0.74258+-0.02829
F1: 0.82502+-0.02080
Best threshold: 0.35500+-0.00000

# semi-siam
# my bert-base-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 15
Accuracy: 0.79416+-0.02254
F1: 0.85159+-0.01872
Best threshold: 0.57100+-0.00134

Epoch: 16
Accuracy: 0.79589+-0.02752
F1: 0.85137+-0.02257
Best threshold: 0.50700+-0.00000

Epoch: 17
Accuracy: 0.79647+-0.02130
F1: 0.85085+-0.01849
Best threshold: 0.59570+-0.00168

####################

############################################################################################################

# semi-siam
# bert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 25
Accuracy: 0.72633+-0.03253
F1: 0.81022+-0.02714
Best threshold: 0.55669+-0.00003

# semi-siam
# bert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression, weights=[1, 2]
Accuracy: 0.74024+-0.03130
F1: 0.81838+-0.02608
Best threshold: 0.65800+-0.00000

# siam triplet, CLS aggreg, hidden 768, tanh, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 22
Accuracy: 0.75186+-0.03756
F1: 0.82794+-0.02836
Best threshold: 0.39157+-0.00009

# siam triplet, mean aggreg, hidden 768, tanh, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 14
Accuracy: 0.74089+-0.02283
F1: 0.81350+-0.02039
Best threshold: 0.30297+-0.00009

# siam triplet, CLS aggreg, tanh, hidden 768, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 12
Accuracy: 0.75013+-0.04282
F1: 0.82393+-0.03117
Best threshold: 0.39966+-0.00012

# siam triplet, CLS aggreg, tanh, hidden 768, tanh, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 12
Accuracy: 0.75013+-0.04282
F1: 0.82393+-0.03117
Best threshold: 0.39919+-0.00003

# siam triplet, CLS aggreg, | hidden 768, tanh, hidden 768, tanh | bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 24
Accuracy: 0.74896+-0.03800
F1: 0.82916+-0.02882
Best threshold: 0.42333+-0.00182

# siam triplet, CLS aggreg, | hidden 768, relu, hidden 768, relu | bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 30
Accuracy: 0.741+-0.03800
F1: 0.818+-0.02882
Best threshold: 0.42333+-0.00182

# siam triplet, CLS aggreg, | empty | bert-base-mrpc-pretrained, lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 3
Accuracy: 0.75070+-0.03011
F1: 0.83144+-0.02465
Best threshold: 0.21172+-0.00018

# siam, CLS aggreg, | empty | bert-base-mrpc-pretrained, lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.74897+-0.03106
F1: 0.82348+-0.02585
Best threshold: 0.08500+-0.00000

# semi-siam, mean aggreg, v, w, |v - w|, v*w, logistic regression, weights=[1, 2], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 23
Accuracy: 0.75709+-0.02742
F1: 0.82491+-0.02230
Best threshold: 0.70960+-0.00945

# semi-siam, CLS aggreg, v, w, |v - w|, v*w, logistic regression, weights=[1, 2], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.77968+-0.02122
F1: 0.84538+-0.01827
Best threshold: 0.62540+-0.00500

Epoch: 15
Accuracy: 0.77970+-0.01969
F1: 0.84165+-0.01722
Best threshold: 0.73130+-0.00287

# semi-siam, CLS aggreg, v, w, |v - w|, v*w, logistic regression, weights=[2, 1], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 18
Accuracy: 0.77969+-0.02061
F1: 0.84223+-0.01812
Best threshold: 0.31130+-0.03657

# semi-siam, CLS aggreg, v, w, |v - w|, v*w, logistic regression, weights=[1, 1], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.77970+-0.02616
F1: 0.84483+-0.02159
Best threshold: 0.47040+-0.00120

Epoch: 18
Accuracy: 0.78029+-0.02628
F1: 0.83889+-0.02162
Best threshold: 0.54620+-0.00098


############# ALBERT ##############
# albert-base-v1
Epoch: 17
Accuracy: 0.83477+-0.01966
F1: 0.88033+-0.01726

# albert-base-v1 with head_pretraining, nesterov, weight_decay=0.01
Epoch: 30
Accuracy: 0.85449+-0.02278
F1: 0.89134+-0.02064

# torch==1.7.1
Epoch: 12
Accuracy: 0.84405+-0.01954
F1: 0.88535+-0.01733

Epoch: 22
Accuracy: 0.84464+-0.02237
F1: 0.88739+-0.01962

Epoch: 26
Accuracy: 0.84753+-0.02168
F1: 0.88923+-0.01723

# albert-base-v2
Epoch: 40
Accuracy: 0.72167+-0.03393
F1: 0.80957+-0.02737

Epoch: 49
Accuracy: 0.72223+-0.03757
F1: 0.80958+-0.03059

# albert-base-v2 with head_pretraining(10 epochs, lr=0.01), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 17
Accuracy: 0.86144+-0.02401
F1: 0.89537+-0.02090

Epoch: 28
Accuracy: 0.86893+-0.02785
F1: 0.90459+-0.02297

# semi-siam
# albert with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 23
Accuracy: 0.72343+-0.03227
F1: 0.81976+-0.02554
Best threshold: 0.43025+-0.00221

Epoch: 26
Accuracy: 0.72924+-0.03053
F1: 0.81494+-0.02598
Best threshold: 0.48515+-0.00138

Epoch: 27
Accuracy: 0.72171+-0.02791
F1: 0.81548+-0.02258
Best threshold: 0.41218+-0.01905

#####################
# CLS aggregation, logistic, empty head
# albert-base-v2 with head_pretraining(10 epochs, lr=0.01), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 21
Accuracy: 0.85740+-0.01698
F1: 0.89377+-0.01215
Best threshold: 0.22330+-0.00269

######################################
# baseline cos_dist for my albert-base
Accuracy: 0.66488+-0.03229
F1: 0.79798+-0.02327
Best threshold: 0.33150+-0.00833

# baseline cos_dist for my albert-base (mean)
Accuracy: 0.68984+-0.02195
F1: 0.78846+-0.02317
Best threshold: 0.07850+-0.01351

# baseline cos_dist for my albert-base (max)
Accuracy: 0.68348+-0.02349
F1: 0.79172+-0.01876
Best threshold: 0.05090+-0.00030

# siam, m = 0.5
# albert-base, lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch:
Accuracy: 0.68049+-0.04065
F1: 0.80010+-0.02810
Best threshold: 0.00200+-0.00000

# siam, m = 0.5
# albert-base, lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 25
Accuracy: 0.70196+-0.04191
F1: 0.81126+-0.02963
Best threshold: 0.04200+-0.00000

Epoch: 30
Accuracy: 0.70080+-0.04321
F1: 0.81037+-0.03053
Best threshold: 0.04490+-0.00030


# siam triplet, m = 0.5
# albert-base lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 7
Accuracy: 0.69624+-0.01796
F1: 0.80346+-0.01460
Best threshold: 0.29610+-0.00939

Epoch: 25
Accuracy: 0.71478+-0.02563
F1: 0.79457+-0.02048
Best threshold: 0.19600+-0.00000

Epoch: 30
Accuracy: 0.70840+-0.02404
F1: 0.79006+-0.01950
Best threshold: 0.19680+-0.00275

# siam triplet, m = 0.5 (mean)
# albert-base lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 10
Accuracy: 0.69445+-0.02463
F1: 0.79324+-0.02052
Best threshold: 0.16520+-0.00060

Epoch: 30
Accuracy: 0.68691+-0.02955
F1: 0.78461+-0.02275
Best threshold: 0.16440+-0.02680


# siam triplet, m = 0.5 (max)
# albert-base lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 8
Accuracy: 0.67648+-0.02560
F1: 0.79662+-0.02005
Best threshold: 0.29460+-0.03582

Epoch: 9
Accuracy: 0.67821+-0.03302
F1: 0.79327+-0.02659
Best threshold: 0.23040+-0.00120

Epoch: 30
Accuracy: 0.66606+-0.02544
F1: 0.78023+-0.02023
Best threshold: 0.17510+-0.01607


# semi-siam
# albert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 24
Accuracy: 0.76924+-0.03106
F1: 0.84305+-0.02455
Best threshold: 0.34360+-0.00474

Epoch: 30
Accuracy: 0.76058+-0.02832
F1: 0.83331+-0.02448
Best threshold: 0.44280+-0.00098


# semi-siam (mean)
# albert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 29
Accuracy: 0.77039+-0.02649
F1: 0.83671+-0.02198
Best threshold: 0.41660+-0.00120

Epoch: 30
Accuracy: 0.76402+-0.02532
F1: 0.82912+-0.02211
Best threshold: 0.62410+-0.02447


# semi-siam (max)
# albert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch:
didn't converge


####################
# baseline cos_dist for my albert-base-pretrained
Accuracy: 0.69216+-0.03420
F1: 0.79886+-0.02791
Best threshold: 0.07930+-0.00287

# siam, m = 0.5
# my albert-base-pretrained, lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 2
Accuracy: 0.73041+-0.02993
F1: 0.82161+-0.02437
Best threshold: 0.06960+-0.00080

Epoch: 30
Accuracy: 0.72635+-0.02410
F1: 0.81566+-0.01925
Best threshold: 0.16810+-0.00324

# siam triplet, m = 0.5
# my albert-base-pretrained lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 7
Accuracy: 0.72402+-0.02446
F1: 0.81882+-0.01856
Best threshold: 0.32100+-0.00000

Epoch: 30
Accuracy: 0.66836+-0.03550
F1: 0.79932+-0.02537
Best threshold: 0.00200+-0.00000

# semi-siam
# my albert-base-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 24
Accuracy: 0.77853+-0.02675
F1: 0.84334+-0.02371
Best threshold: 0.35030+-0.00283

Epoch: 30
Accuracy: 0.77447+-0.02667
F1: 0.82944+-0.02554
Best threshold: 0.55600+-0.05389

########### ROBERTA ###############
# roberta-base
Epoch: 15
Accuracy: 0.83650+-0.02539
F1: 0.87950+-0.02133

Epoch: 17
Accuracy: 0.83825+-0.02181
F1: 0.87582+-0.02011

Epoch: 20
Accuracy: 0.85505+-0.01738
F1: 0.89114+-0.01604

# roberta with head_pretraining, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 17
Accuracy: 0.87070+-0.02348
F1: 0.90262+-0.01990

Epoch: 23
Accuracy: 0.87883+-0.01914
F1: 0.90957+-0.01659

# torch==1.7.1
Epoch: 24
Accuracy: 0.86377+-0.02332
F1: 0.89898+-0.01909

Epoch: 32
Accuracy: 0.86551+-0.03079
F1: 0.90037+-0.02396

Epoch: 40
Accuracy: 0.86723+-0.02963
F1: 0.90236+-0.02267

# torch==1.7.1 (2)
Epoch: 26
Accuracy: 0.86375+-0.02727
F1: 0.89976+-0.02127

Epoch: 40
Accuracy: 0.86723+-0.02963
F1: 0.90236+-0.02267

# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 26
Accuracy: 0.87011+-0.02384
F1: 0.90296+-0.02087

# semi-siam
# roberta with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 16
Accuracy: 0.67530+-0.03343
F1: 0.80270+-0.02409
Best threshold: 0.57670+-0.00020

#########################################
# CLS aggregation, logistic, empty head
# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 30
Accuracy: 0.79591+-0.01589
F1: 0.85534+-0.01295
Best threshold: 0.37400+-0.01868


# CLS aggregation, logistic, empty head
# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 30
Accuracy: 0.85100+-0.01567
F1: 0.88711+-0.01530
Best threshold: 0.75080+-0.00060


# CLS aggregation, logistic, empty head
# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.0001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 26
Accuracy: 0.86546+-0.02176
F1: 0.89726+-0.01931
Best threshold: 0.81420+-0.00040

Epoch: 30
Accuracy: 0.86490+-0.02359
F1: 0.89815+-0.01975
Best threshold: 0.78820+-0.00160

# CLS aggregation, logistic, empty head, roberta_base_cls_20210401-180654.pt
# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 23
Accuracy: 0.88807+-0.02239
F1: 0.91543+-0.01934
Best threshold: 0.50270+-0.00090

Epoch: 30
Accuracy: 0.88521+-0.01907
F1: 0.91415+-0.01583
Best threshold: 0.32560+-0.00680

####################
# baseline cos_dist for roberta-base
Accuracy: 0.66372+-0.03108
F1: 0.79718+-0.02240
Best threshold: 0.00570+-0.00046

# siam, m = 0.5
# roberta-base, lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 29
Accuracy: 0.66604+-0.03438
F1: 0.79867+-0.02440
Best threshold: 0.12780+-0.00660

Epoch: 30
Accuracy: 0.66139+-0.03620
F1: 0.79545+-0.02641
Best threshold: 0.18550+-0.07740


# siam triplet, m = 0.5
# roberta-base lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 27
Accuracy: 0.71998+-0.02320
F1: 0.81231+-0.01807
Best threshold: 0.35300+-0.00000

Epoch: 30
Accuracy: 0.71129+-0.02207
F1: 0.80532+-0.01832
Best threshold: 0.35040+-0.01260

# semi-siam
# roberta-base with head_pretraining(10 epochs, lr=0.0001), lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 22
Accuracy: 0.80110+-0.02846
F1: 0.86207+-0.02420
Best threshold: 0.40820+-0.00240

Epoch: 25
Accuracy: 0.80867+-0.03452
F1: 0.86012+-0.02727
Best threshold: 0.59640+-0.02836

Epoch: 27
Accuracy: 0.80636+-0.02124
F1: 0.86095+-0.01815
Best threshold: 0.45660+-0.02143

Epoch: 30
Accuracy: 0.80401+-0.03360
F1: 0.85778+-0.02832
Best threshold: 0.45100+-0.01963

####################
# baseline cos_dist for my roberta-base-pretrained
Accuracy: 0.72926+-0.03170
F1: 0.80681+-0.02473
Best threshold: 0.01570+-0.00046

# siam, m = 0.5
# my roberta-base-pretrained, lr=0.0001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 30
Accuracy: 0.66371+-0.03382
F1: 0.79649+-0.02469
Best threshold: 0.06630+-0.00494


# siam, m = 0.5
# my roberta-base-pretrained, lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 3
Accuracy: 0.66719+-0.03621
F1: 0.79880+-0.02546
Best threshold: 0.05230+-0.00090

Epoch: 30
Accuracy: 0.66488+-0.03304
F1: 0.79766+-0.02394
Best threshold: 0.02720+-0.00183



# siam triplet, m = 0.5
# my roberta-base-pretrained lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 15
Accuracy: 0.72462+-0.01933
F1: 0.81663+-0.01645
Best threshold: 0.34660+-0.00180

Epoch: 30
Accuracy: 0.71826+-0.02242
F1: 0.79932+-0.01807
Best threshold: 0.29520+-0.00328


# semi-siam
# my roberta-base-pretrained with head_pretraining(10 epochs, lr=0.00001), lr=0.0005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 30
Accuracy: 0.79766+-0.02677
F1: 0.85460+-0.02202
Best threshold: 0.50600+-0.00000



