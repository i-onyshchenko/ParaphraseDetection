torch==1.3.0

# siamese with one hidden layer (768) and tanh activation
# mean of ALL embeddings
# Epoch: 18
Accuracy: 0.76690+-0.03268
F1: 0.83254+-0.02580

# CLS classification with sigmoid on bert-base-cased-mrpc (w/o dropout)
Epoch:1
Accuracy: 0.83008+-0.03464
F1: 0.87448+-0.02778

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 768, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83182+-0.03472
F1: 0.87617+-0.02747

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83298+-0.03584
F1: 0.87697+-0.02840


# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 512
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:2
Accuracy: 0.83298+-0.03479
F1: 0.87696+-0.02773

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 16, base_model os trainable, batch_size=16, epochs=100, epoch_size=16
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 18
Accuracy: 0.83818+-0.03178
F1: 0.88113+-0.02572

Epoch: 25
Accuracy: 0.84340+-0.03496
F1: 0.88564+-0.02730


# CLS classification with sigmoid on bert-base-cased_mrpc, hidden = 768, base_model os trainable, batch_size=12, epochs=100, epoch_size=80
mean of logits [s1, s2] and [s2, s1]
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 11
Accuracy: 0.84981+-0.02309
F1: 0.88718+-0.01881

Epoch: 14
Accuracy: 0.85157+-0.01803
F1: 0.88886+-0.01670

Epoch: 24
Accuracy: 0.85039+-0.02438
F1: 0.89079+-0.01925

Epoch: 36
Accuracy: 0.85039+-0.02132
F1: 0.88936+-0.01855

# fixed seed 666
Epoch: 9
Accuracy: 0.85444+-0.02743
F1: 0.89463+-0.02125

# bert-base-cased
Epoch: 13
Accuracy: 0.85213+-0.02504
F1: 0.88925+-0.02071

Epoch: 18
Accuracy: 0.85097+-0.02680
F1: 0.89042+-0.02160

Epoch: 25
Accuracy: 0.85270+-0.02480
F1: 0.89090+-0.01978

# after update of transformers
Epoch: 14
Accuracy: 0.85444+-0.03050
F1: 0.89368+-0.02368

# bert-base with head_pretraining (same sched and optim),lr=0.01, momentum=0.9, nesterov, weight_decay=0.01
Epoch: 19
Accuracy: 0.85910+-0.02104
F1: 0.89200+-0.01820

Epoch: 21
Accuracy: 0.86025+-0.02087
F1: 0.89232+-0.01905

# bert-base-v2 with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch:

############# ALBERT ##############
# albert-base-v1
Epoch: 17
Accuracy: 0.83477+-0.01966
F1: 0.88033+-0.01726

# albert-base-v1 with head_pretraining, nesterov, weight_decay=0.01
Epoch: 30
Accuracy: 0.85449+-0.02278
F1: 0.89134+-0.02064

# torch==1.7.1
Epoch: 12
Accuracy: 0.84405+-0.01954
F1: 0.88535+-0.01733

Epoch: 22
Accuracy: 0.84464+-0.02237
F1: 0.88739+-0.01962

Epoch: 26
Accuracy: 0.84753+-0.02168
F1: 0.88923+-0.01723

# albert-base-v2
Epoch: 40
Accuracy: 0.72167+-0.03393
F1: 0.80957+-0.02737

Epoch: 49
Accuracy: 0.72223+-0.03757
F1: 0.80958+-0.03059

# albert-base-v2 with head_pretraining(10 epochs, lr=0.01), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 17
Accuracy: 0.86144+-0.02401
F1: 0.89537+-0.02090

Epoch: 28
Accuracy: 0.86893+-0.02785
F1: 0.90459+-0.02297

########### ROBERTA ###############
# roberta-base
Epoch: 15
Accuracy: 0.83650+-0.02539
F1: 0.87950+-0.02133

Epoch: 17
Accuracy: 0.83825+-0.02181
F1: 0.87582+-0.02011

Epoch: 20
Accuracy: 0.85505+-0.01738
F1: 0.89114+-0.01604

# roberta with head_pretraining, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 17
Accuracy: 0.87070+-0.02348
F1: 0.90262+-0.01990

Epoch: 23
Accuracy: 0.87883+-0.01914
F1: 0.90957+-0.01659

# torch==1.7.1
Epoch: 24
Accuracy: 0.86377+-0.02332
F1: 0.89898+-0.01909

Epoch: 32
Accuracy: 0.86551+-0.03079
F1: 0.90037+-0.02396

Epoch: 40
Accuracy: 0.86723+-0.02963
F1: 0.90236+-0.02267

# torch==1.7.1 (2)
Epoch: 26
Accuracy: 0.86375+-0.02727
F1: 0.89976+-0.02127

Epoch: 40
Accuracy: 0.86723+-0.02963
F1: 0.90236+-0.02267

# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 27
Accuracy: 0.87011+-0.02384
F1: 0.90296+-0.02087