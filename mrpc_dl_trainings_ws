torch==1.3.0

# siamese with one hidden layer (768) and tanh activation
# mean of ALL embeddings
# Epoch: 18
Accuracy: 0.76690+-0.03268
F1: 0.83254+-0.02580

# CLS classification with sigmoid on bert-base-cased-mrpc (w/o dropout)
Epoch:1
Accuracy: 0.83008+-0.03464
F1: 0.87448+-0.02778

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 768, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83182+-0.03472
F1: 0.87617+-0.02747

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 256
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:1
Accuracy: 0.83298+-0.03584
F1: 0.87697+-0.02840


# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 512
        x = torch.mean(inputs[:, :1], dim=1)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.hidden_fc(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch:2
Accuracy: 0.83298+-0.03479
F1: 0.87696+-0.02773

# CLS classification with sigmoid on bert-base-cased-mrpc, hidden = 1024, bs = 16, base_model os trainable, batch_size=16, epochs=100, epoch_size=16
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 18
Accuracy: 0.83818+-0.03178
F1: 0.88113+-0.02572

Epoch: 25
Accuracy: 0.84340+-0.03496
F1: 0.88564+-0.02730


# CLS classification with sigmoid on bert-base-cased_mrpc, hidden = 768, base_model is trainable, batch_size=12, epochs=100, epoch_size=80
mean of logits [s1, s2] and [s2, s1]
x = inputs[:, 0]
        # x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm1(x)
        x = self.hidden_fc(x)
        x = torch.relu(x)
        # x = self.dropout(x)
        # x = self.batch_norm2(x)
        x = self.output_layer(x)

        x = torch.sigmoid(x)
Epoch: 11
Accuracy: 0.84981+-0.02309
F1: 0.88718+-0.01881

Epoch: 14
Accuracy: 0.85157+-0.01803
F1: 0.88886+-0.01670

Epoch: 24
Accuracy: 0.85039+-0.02438
F1: 0.89079+-0.01925

Epoch: 36
Accuracy: 0.85039+-0.02132
F1: 0.88936+-0.01855

# fixed seed 666
Epoch: 9
Accuracy: 0.85444+-0.02743
F1: 0.89463+-0.02125

# bert-base-cased
Epoch: 13
Accuracy: 0.85213+-0.02504
F1: 0.88925+-0.02071

Epoch: 18
Accuracy: 0.85097+-0.02680
F1: 0.89042+-0.02160

Epoch: 25
Accuracy: 0.85270+-0.02480
F1: 0.89090+-0.01978

# after update of transformers
Epoch: 14
Accuracy: 0.85444+-0.03050
F1: 0.89368+-0.02368

# bert-base with head_pretraining (same sched and optim),lr=0.01, momentum=0.9, nesterov, weight_decay=0.01
Epoch: 19
Accuracy: 0.85910+-0.02104
F1: 0.89200+-0.01820

Epoch: 21
Accuracy: 0.86025+-0.02087
F1: 0.89232+-0.01905

# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 28
Accuracy: 0.86201+-0.02340
F1: 0.89648+-0.02057

# mean aggregation
# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9


# max aggregation
# bert-base with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 27
Accuracy: 0.85969+-0.02440
F1: 0.89701+-0.02076

# semi-siam
# bert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 25
Accuracy: 0.72633+-0.03253
F1: 0.81022+-0.02714
Best threshold: 0.55669+-0.00003

# semi-siam
# bert-base with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression, weights=[1, 2]
Accuracy: 0.74024+-0.03130
F1: 0.81838+-0.02608
Best threshold: 0.65800+-0.00000

# siam triplet, CLS aggreg, hidden 768, tanh, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 22
Accuracy: 0.75186+-0.03756
F1: 0.82794+-0.02836
Best threshold: 0.39157+-0.00009

# siam triplet, mean aggreg, hidden 768, tanh, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 14
Accuracy: 0.74089+-0.02283
F1: 0.81350+-0.02039
Best threshold: 0.30297+-0.00009

# siam triplet, CLS aggreg, tanh, hidden 768, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 12
Accuracy: 0.75013+-0.04282
F1: 0.82393+-0.03117
Best threshold: 0.39966+-0.00012

# siam triplet, CLS aggreg, tanh, hidden 768, tanh, bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 12
Accuracy: 0.75013+-0.04282
F1: 0.82393+-0.03117
Best threshold: 0.39919+-0.00003

# siam triplet, CLS aggreg, | hidden 768, tanh, hidden 768, tanh | bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 24
Accuracy: 0.74896+-0.03800
F1: 0.82916+-0.02882
Best threshold: 0.42333+-0.00182

# siam triplet, CLS aggreg, | hidden 768, relu, hidden 768, relu | bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 30
Accuracy: 0.741+-0.03800
F1: 0.818+-0.02882
Best threshold: 0.42333+-0.00182

# siam triplet, CLS aggreg, | empty | bert-base-mrpc-pretrained, lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 3
Accuracy: 0.75070+-0.03011
F1: 0.83144+-0.02465
Best threshold: 0.21172+-0.00018

# siam, CLS aggreg, | empty | bert-base-mrpc-pretrained, lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch:

# semi-siam, mean aggreg, v, w, |v - w|, v*w, logistic regression, weights=[1, 2], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 23
Accuracy: 0.75709+-0.02742
F1: 0.82491+-0.02230
Best threshold: 0.70960+-0.00945

# semi-siam, CLS aggreg, v, w, |v - w|, v*w, logistic regression, weights=[1, 2], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.77968+-0.02122
F1: 0.84538+-0.01827
Best threshold: 0.62540+-0.00500

Epoch: 15
Accuracy: 0.77970+-0.01969
F1: 0.84165+-0.01722
Best threshold: 0.73130+-0.00287

# semi-siam, CLS aggreg, v, w, |v - w|, v*w, logistic regression, weights=[2, 1], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 18
Accuracy: 0.77969+-0.02061
F1: 0.84223+-0.01812
Best threshold: 0.31130+-0.03657

# semi-siam, CLS aggreg, v, w, |v - w|, v*w, logistic regression, weights=[1, 1], bert-base-mrpc-pretrained with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 13
Accuracy: 0.77970+-0.02616
F1: 0.84483+-0.02159
Best threshold: 0.47040+-0.00120

Epoch: 18
Accuracy: 0.78029+-0.02628
F1: 0.83889+-0.02162
Best threshold: 0.54620+-0.00098


############# ALBERT ##############
# albert-base-v1
Epoch: 17
Accuracy: 0.83477+-0.01966
F1: 0.88033+-0.01726

# albert-base-v1 with head_pretraining, nesterov, weight_decay=0.01
Epoch: 30
Accuracy: 0.85449+-0.02278
F1: 0.89134+-0.02064

# torch==1.7.1
Epoch: 12
Accuracy: 0.84405+-0.01954
F1: 0.88535+-0.01733

Epoch: 22
Accuracy: 0.84464+-0.02237
F1: 0.88739+-0.01962

Epoch: 26
Accuracy: 0.84753+-0.02168
F1: 0.88923+-0.01723

# albert-base-v2
Epoch: 40
Accuracy: 0.72167+-0.03393
F1: 0.80957+-0.02737

Epoch: 49
Accuracy: 0.72223+-0.03757
F1: 0.80958+-0.03059

# albert-base-v2 with head_pretraining(10 epochs, lr=0.01), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 17
Accuracy: 0.86144+-0.02401
F1: 0.89537+-0.02090

Epoch: 28
Accuracy: 0.86893+-0.02785
F1: 0.90459+-0.02297

# semi-siam
# albert with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 23
Accuracy: 0.72343+-0.03227
F1: 0.81976+-0.02554
Best threshold: 0.43025+-0.00221

Epoch: 26
Accuracy: 0.72924+-0.03053
F1: 0.81494+-0.02598
Best threshold: 0.48515+-0.00138

Epoch: 27
Accuracy: 0.72171+-0.02791
F1: 0.81548+-0.02258
Best threshold: 0.41218+-0.01905

########### ROBERTA ###############
# roberta-base
Epoch: 15
Accuracy: 0.83650+-0.02539
F1: 0.87950+-0.02133

Epoch: 17
Accuracy: 0.83825+-0.02181
F1: 0.87582+-0.02011

Epoch: 20
Accuracy: 0.85505+-0.01738
F1: 0.89114+-0.01604

# roberta with head_pretraining, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 17
Accuracy: 0.87070+-0.02348
F1: 0.90262+-0.01990

Epoch: 23
Accuracy: 0.87883+-0.01914
F1: 0.90957+-0.01659

# torch==1.7.1
Epoch: 24
Accuracy: 0.86377+-0.02332
F1: 0.89898+-0.01909

Epoch: 32
Accuracy: 0.86551+-0.03079
F1: 0.90037+-0.02396

Epoch: 40
Accuracy: 0.86723+-0.02963
F1: 0.90236+-0.02267

# torch==1.7.1 (2)
Epoch: 26
Accuracy: 0.86375+-0.02727
F1: 0.89976+-0.02127

Epoch: 40
Accuracy: 0.86723+-0.02963
F1: 0.90236+-0.02267

# roberta with head_pretraining(10 epochs, lr=0.01), lr=0.001, nesterov, weight_decay=0.01, momentum=0.9
Epoch: 26
Accuracy: 0.87011+-0.02384
F1: 0.90296+-0.02087

# semi-siam
# albert with head_pretraining(10 epochs, lr=0.0005), lr=0.00005, nesterov, weight_decay=0.01, momentum=0.9, v, w, |v - w|, v*w, logistic regression
Epoch: 16
Accuracy: 0.67530+-0.03343
F1: 0.80270+-0.02409
Best threshold: 0.57670+-0.00020